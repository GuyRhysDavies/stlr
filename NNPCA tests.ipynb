{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b1b3300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as onp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "376a71d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import jax.numpy as np\n",
    "from jax import grad, jit, vmap, value_and_grad\n",
    "from jax.nn import relu\n",
    "import optax \n",
    "\n",
    "from jax import random\n",
    "\n",
    "# Generate key which is used to generate random numbers\n",
    "key = random.PRNGKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "96fa8e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pcann():\n",
    "    def __init__(self, layer_sizes=[5, 5, 2], key=random.PRNGKey(1)):\n",
    "        self.params = self.initialize_mlp(layer_sizes, key)\n",
    "        self.batch_forward = vmap(self.forward_pass, in_axes=(None, 0))\n",
    "        self.initialize_opt()\n",
    "        \n",
    "    def train(self, xs, ys, epochs=100):\n",
    "        for _ in range(epochs):\n",
    "            grads = jax.grad(self.loss)(self.params, xs, ys)\n",
    "            updates, self.opt_state = self.optimizer.update(grads, self.opt_state)\n",
    "            self.params = optax.apply_updates(self.params, updates)\n",
    "            print(f'Loss is {self.loss(self.params, xs, ys)}')\n",
    "        \n",
    "    def initialize_opt(self, lr=1e-1):\n",
    "        self.optimizer = optax.adam(lr)\n",
    "        self.opt_state = self.optimizer.init(self.params)\n",
    "        \n",
    "    def loss(self, params, x, y):\n",
    "        preds = self.batch_forward(params, x)\n",
    "        loss = np.mean(optax.l2_loss(preds, y))\n",
    "        return loss\n",
    "        \n",
    "    def forward_pass(self, params, x):\n",
    "        \"\"\" Compute the forward pass for each example individually \"\"\"\n",
    "        activations = x\n",
    "\n",
    "        # Loop over the ReLU hidden layers\n",
    "        for w, b in params[:-1]:\n",
    "            outputs = np.dot(w, activations) + b  # apply affine transformation\n",
    "            activations = relu(outputs)  #  apply nonlinear activation\n",
    "\n",
    "        # Perform final trafo to logits\n",
    "        final_w, final_b = params[-1]\n",
    "        final_outputs = np.sum(np.dot(final_w, activations) + final_b)\n",
    "        return final_outputs\n",
    "        \n",
    "    def initialize_mlp(self, sizes, key):\n",
    "        \"\"\" Initialize the weights of all layers of a linear layer network \"\"\"\n",
    "        keys = random.split(key, len(sizes))\n",
    "        # Initialize a single layer with Gaussian weights -  helper function\n",
    "        def initialize_layer(m, n, key, scale=1e-2):\n",
    "            w_key, b_key = random.split(key)\n",
    "            return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
    "        return [initialize_layer(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2c779f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(42)\n",
    "target_params = 0.5\n",
    "\n",
    "# Generate some data.\n",
    "xs = jax.random.normal(key, (100, 5))\n",
    "ys = np.sum(xs * target_params, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "48d093b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is 0.4158563017845154\n",
      "Loss is 0.17924073338508606\n",
      "Loss is 0.037840694189071655\n",
      "Loss is 0.13168497383594513\n",
      "Loss is 0.13468925654888153\n",
      "Loss is 0.04993642121553421\n",
      "Loss is 0.01779324933886528\n",
      "Loss is 0.04430382326245308\n",
      "Loss is 0.07623517513275146\n",
      "Loss is 0.07979195564985275\n",
      "Loss is 0.057037800550460815\n",
      "Loss is 0.027858078479766846\n",
      "Loss is 0.013929243199527264\n",
      "Loss is 0.02245417609810829\n",
      "Loss is 0.03732893243432045\n",
      "Loss is 0.0383341908454895\n",
      "Loss is 0.02478916384279728\n",
      "Loss is 0.011566661298274994\n",
      "Loss is 0.009864678606390953\n",
      "Loss is 0.016465405002236366\n",
      "Loss is 0.021581336855888367\n",
      "Loss is 0.02024519443511963\n",
      "Loss is 0.01443567592650652\n",
      "Loss is 0.009171809069812298\n",
      "Loss is 0.007160389330238104\n",
      "Loss is 0.007956251502037048\n",
      "Loss is 0.009236621670424938\n",
      "Loss is 0.00870217103511095\n",
      "Loss is 0.006757930386811495\n",
      "Loss is 0.00557346548885107\n",
      "Loss is 0.005530070513486862\n",
      "Loss is 0.0055364458821713924\n",
      "Loss is 0.0046552554704248905\n",
      "Loss is 0.003405258059501648\n",
      "Loss is 0.0029534483328461647\n",
      "Loss is 0.003676404943689704\n",
      "Loss is 0.004418535158038139\n",
      "Loss is 0.0037809095811098814\n",
      "Loss is 0.002071915427222848\n",
      "Loss is 0.0009994023712351918\n",
      "Loss is 0.0013974730391055346\n",
      "Loss is 0.0023944666609168053\n",
      "Loss is 0.002663952996954322\n",
      "Loss is 0.0019085699459537864\n",
      "Loss is 0.0009453640086576343\n",
      "Loss is 0.0006475776899605989\n",
      "Loss is 0.001008775201626122\n",
      "Loss is 0.0013321005972102284\n",
      "Loss is 0.0011583705199882388\n",
      "Loss is 0.0007859483011998236\n",
      "Loss is 0.0006451211520470679\n",
      "Loss is 0.0007036721799522638\n",
      "Loss is 0.0006740581011399627\n",
      "Loss is 0.0004991658497601748\n",
      "Loss is 0.00039790841401554644\n",
      "Loss is 0.0004926633555442095\n",
      "Loss is 0.000586431473493576\n",
      "Loss is 0.00046675660996697843\n",
      "Loss is 0.00023281773610506207\n",
      "Loss is 0.00014517182717099786\n",
      "Loss is 0.0002493809151928872\n",
      "Loss is 0.0003517557925079018\n",
      "Loss is 0.0003052531974390149\n",
      "Loss is 0.00018264092796016484\n",
      "Loss is 0.0001319786097155884\n",
      "Loss is 0.00016327208140864968\n",
      "Loss is 0.00017408357234671712\n",
      "Loss is 0.00013435757136903703\n",
      "Loss is 0.00011307284876238555\n",
      "Loss is 0.00013643744750879705\n",
      "Loss is 0.00014502857811748981\n",
      "Loss is 0.0001019508927129209\n",
      "Loss is 5.643002441502176e-05\n",
      "Loss is 6.431630026781932e-05\n",
      "Loss is 9.987188241211697e-05\n",
      "Loss is 0.00010290572390658781\n",
      "Loss is 6.959558231756091e-05\n",
      "Loss is 4.729460852104239e-05\n",
      "Loss is 5.292300556902774e-05\n",
      "Loss is 5.9063429944217205e-05\n",
      "Loss is 5.033616616856307e-05\n",
      "Loss is 4.42802811448928e-05\n",
      "Loss is 5.145991963217966e-05\n",
      "Loss is 5.445790884550661e-05\n",
      "Loss is 4.07981569878757e-05\n",
      "Loss is 2.6731806428870186e-05\n",
      "Loss is 2.9176793759688735e-05\n",
      "Loss is 3.8602880522375926e-05\n",
      "Loss is 3.790296977967955e-05\n",
      "Loss is 2.941393540822901e-05\n",
      "Loss is 2.5507491955067962e-05\n",
      "Loss is 2.658779703779146e-05\n",
      "Loss is 2.4689577912795357e-05\n",
      "Loss is 2.0615327230188996e-05\n",
      "Loss is 2.0995246813981794e-05\n",
      "Loss is 2.3988441171240993e-05\n",
      "Loss is 2.2472593627753668e-05\n",
      "Loss is 1.729433824948501e-05\n",
      "Loss is 1.5346538930316456e-05\n",
      "Loss is 1.7468828445998952e-05\n"
     ]
    }
   ],
   "source": [
    "nn = pcann()\n",
    "nn.train(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32cef7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
